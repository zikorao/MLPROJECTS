{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqdQbHoDyQUx","outputId":"34f803a0-3c13-419a-d40f-743d1c4c6952"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vPA1tvKa_qjg"},"source":["### Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5gOC731yUcA","outputId":"a5e7f7cb-4d13-4d94-be1b-1b1278c2156f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total images loaded: 20000\n"]}],"source":["import os\n","import numpy as np\n","\n","def load_images_from_batches(batch_folder):\n","    \"\"\"\n","    Load processed images from batch files and store them in a dictionary.\n","\n","    Args:\n","        batch_folder (str): Path to the folder containing the saved batch files.\n","\n","    Returns:\n","        image_dict (dict): Dictionary mapping image ID (integer) to preprocessed image data.\n","    \"\"\"\n","    image_dict = {}\n","\n","    # Loop through all files in the batch folder\n","    for batch_file in os.listdir(batch_folder):\n","        if batch_file.endswith('.npz'):\n","            batch_path = os.path.join(batch_folder, batch_file)\n","\n","            # Load the batch file\n","            batch_data = np.load(batch_path)\n","            images = batch_data['images']\n","            filenames = batch_data['filenames']\n","\n","            # Store each image and its corresponding image ID in the dictionary (convert image_id to int)\n","            for i, image_id in enumerate(filenames):\n","                image_id_int = int(image_id)  # Convert image ID to integer to remove leading zeros\n","                image_dict[image_id_int] = images[i]\n","\n","    return image_dict\n","\n","# Path where the batch files are stored\n","batch_folder = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/Processed Images Train'\n","\n","# Load the images into a dictionary\n","image_dict = load_images_from_batches(batch_folder)\n","\n","# Print the number of images loaded\n","print(f\"Total images loaded: {len(image_dict)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUVNDFnezO9V"},"outputs":[],"source":["import pickle\n","import numpy as np\n","\n","\n","# Load tokenized data (train_x and test_x) from pickle files\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/tokenized_train_y.pkl', 'rb') as f:\n","    train_y = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/tokenized_test_y.pkl', 'rb') as f:\n","    test_y = pickle.load(f)\n","\n","# Load tokenized data (train_x and test_x) from pickle files\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/tokenized_train_x.pkl', 'rb') as f:\n","    train_x = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/tokenized_test_x.pkl', 'rb') as f:\n","    test_x = pickle.load(f)\n","\n","#bring confidence\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/processed_train_y_conf.pkl', 'rb') as f:\n","    train_y_cof = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Project/processed_test_y_conf.pkl', 'rb') as f:\n","    test_y_cof = pickle.load(f)\n","\n","train_y['confidence_scores']=train_y_cof['confidence_scores']\n","test_y['confidence_scores']=test_y_cof['confidence_scores']\n"]},{"cell_type":"markdown","metadata":{"id":"vZ-G6pPX_hqU"},"source":["### OLD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FT73lfmG-uIv"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertModel\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision import transforms, models\n","from torchvision.models import VGG16_Weights\n","\n","###########################################\n","# Custom Dataset Class for Loading VQA Data\n","###########################################\n","\n","class VQADataset(Dataset):\n","    def __init__(self, data_x, data_y, image_dict, augment=False, augment_prob=0.3, is_training=True):\n","        self.data_x = data_x\n","        self.data_y = data_y\n","        self.image_dict = image_dict\n","        self.augment = augment\n","        self.augment_prob = augment_prob\n","        self.is_training = is_training\n","\n","        self.train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(5),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","            transforms.RandomResizedCrop(224, scale=(0.95, 1.0))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","\n","    def __getitem__(self, idx):\n","        input_ids_list = torch.tensor(self.data_x['input_ids'].iloc[idx], dtype=torch.long)\n","        attention_mask_list = torch.tensor(self.data_x['attention_mask'].iloc[idx], dtype=torch.long)\n","        image_id = self.data_x['image_id'].iloc[idx]\n","        image_tensor = torch.tensor(self.image_dict[image_id], dtype=torch.float32).permute(2, 0, 1)\n","\n","        if self.augment and torch.rand(1).item() < self.augment_prob:\n","            image_tensor = self.train_transform(image_tensor)\n","\n","        label = torch.tensor(self.data_y['label'].iloc[idx], dtype=torch.long)\n","\n","        if self.is_training:\n","            confidence_scores = torch.tensor(self.data_y['confidence_scores'].iloc[idx], dtype=torch.float32)\n","            return input_ids_list, attention_mask_list, image_tensor, label, confidence_scores\n","        else:\n","            confidence_scores = torch.zeros(len(self.data_x['input_ids'].iloc[idx]), dtype=torch.float32)\n","            correct_answer_idx = self.data_y['label'].iloc[idx]\n","            confidence_scores[correct_answer_idx] = 1.0\n","            return input_ids_list, attention_mask_list, image_tensor, label, confidence_scores\n","\n","###########################################\n","# BERT-based Text Model with Freezing up to Layer 8\n","###########################################\n","\n","class TextModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune=False, reduced_dim=512):\n","        super(TextModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # Freeze the first 8 layers of BERT\n","        for param in self.bert.encoder.layer[:8].parameters():\n","            param.requires_grad = False\n","\n","        # Dimensionality reduction after BERT\n","        self.fc_reduce = nn.Linear(768, reduced_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        batch_size, num_choices, seq_len = input_ids.size()\n","        input_ids = input_ids.view(-1, seq_len)\n","        attention_mask = attention_mask.view(-1, seq_len)\n","\n","        text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_features = text_output.last_hidden_state[:, 0, :]\n","\n","        reduced_text_features = self.fc_reduce(text_features)\n","        reduced_text_features = reduced_text_features.view(batch_size, num_choices, -1)\n","        return reduced_text_features\n","\n","###########################################\n","# Image Model using VGG-16 with Customization\n","###########################################\n","\n","class ImageModel(nn.Module):\n","    def __init__(self, fine_tune_layers=True, reduced_dim=512):\n","        super(ImageModel, self).__init__()\n","        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","        # Freeze VGG layers by default\n","        for param in self.vgg16.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreeze the last layers for fine-tuning\n","        if fine_tune_layers:\n","            for param in list(self.vgg16.features.parameters())[-5:]:\n","                param.requires_grad = True\n","\n","        self.fc_reduce = nn.Linear(25088, reduced_dim)\n","\n","    def forward(self, image_tensor):\n","        image_features = self.vgg16.features(image_tensor)\n","        image_features = torch.flatten(image_features, start_dim=1)\n","        image_features = self.fc_reduce(image_features)\n","        return image_features\n","\n","###########################################\n","# Fusion Model combining Text and Image Features\n","###########################################\n","\n","class FusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(FusionModel, self).__init__()\n","\n","        self.text_model = TextModel(bert_model_name=bert_model_name, fine_tune=fine_tune_bert)\n","        self.image_model = ImageModel(fine_tune_layers=fine_tune_layers, reduced_dim=reduced_dim)\n","\n","        # Fusion layer\n","        self.fusion_layer = nn.Linear(reduced_dim + reduced_dim, 64)\n","        self.batch_norm = nn.BatchNorm1d(64)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n","        image_features = self.image_model(image_tensor=image_tensor)\n","\n","        batch_size, num_choices, _ = text_features.size()\n","        image_features = image_features.unsqueeze(1).expand(batch_size, num_choices, -1)\n","\n","        combined_features = torch.cat((text_features, image_features), dim=2)\n","        combined_features = combined_features.view(-1, combined_features.size(2))\n","        fused_features = self.fusion_layer(combined_features)\n","        fused_features = self.batch_norm(fused_features)\n","        fused_features = self.dropout(fused_features)\n","        fused_features = fused_features.view(batch_size, num_choices, -1)\n","\n","        return fused_features\n","\n","###########################################\n","# VQA Model for Final Prediction\n","###########################################\n","\n","class VQA_Model(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(VQA_Model, self).__init__()\n","        self.fusion_model = FusionModel(bert_model_name=bert_model_name, fine_tune_bert=fine_tune_bert, fine_tune_layers=fine_tune_layers, dropout_prob=dropout_prob, reduced_dim=reduced_dim)\n","        self.classifier = nn.Linear(64, 1)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor, confidence_scores=None):\n","        fused_features = self.fusion_model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_tensor)\n","        logits = self.classifier(fused_features).squeeze(-1)\n","\n","        # Adjust logits with confidence scores during training (if provided)\n","        if confidence_scores is not None:\n","            logits = logits * confidence_scores\n","\n","        batch_size, num_choices = input_ids.size(0), fused_features.size(1)\n","        logits = logits.view(batch_size, num_choices)\n","\n","        return logits\n","\n","###########################################\n","# Training Function\n","###########################################\n","\n","def train_model(model, train_dataloader, val_dataloader, num_epochs=10, optimizer=None, checkpoint_path='model_checkpoint.pt'):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    if optimizer is None:\n","        raise ValueError(\"Optimizer must be provided\")\n","\n","    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    torch.backends.cudnn.benchmark = True  # Enable cuDNN autotuning\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","            input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","            input_ids, attention_mask, image_tensor, labels, confidence_scores = (\n","                input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","            )\n","\n","            optimizer.zero_grad()\n","\n","            logits = model(input_ids, attention_mask, image_tensor, confidence_scores=confidence_scores)\n","            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n","            loss = loss_fn(logits, labels)\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(logits, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        avg_loss = total_loss / len(train_dataloader)\n","        accuracy = 100 * correct / total\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","        # Save model after each epoch\n","        checkpoint_file = f\"{checkpoint_path}_epoch_{epoch + 1}.pt\"\n","        torch.save(model.state_dict(), checkpoint_file)\n","        print(f\"Model saved to {checkpoint_file}\")\n","\n","        # Only run validation every 3 epochs\n","        if (epoch + 1) % 3 == 0:\n","            model.eval()\n","            val_loss = 0\n","            val_correct = 0\n","            val_total = 0\n","            with torch.no_grad():\n","                for batch in val_dataloader:\n","                    input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","                    input_ids, attention_mask, image_tensor, labels, confidence_scores = (\n","                        input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","                    )\n","                    logits = model(input_ids, attention_mask, image_tensor)\n","                    val_loss += loss_fn(logits, labels).item()\n","\n","                    _, predicted = torch.max(logits, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","            avg_val_loss = val_loss / len(val_dataloader)\n","            val_accuracy = 100 * val_correct / val_total\n","            print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","            scheduler.step()\n","\n","    print(\"Training complete.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vn_N0MShNsnF"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertModel\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torchvision import transforms, models\n","from torchvision.models import VGG16_Weights\n","from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n","import torch.autograd.profiler as profiler  # For profiling\n","\n","###########################################\n","# Custom Dataset Class for Loading VQA Data\n","###########################################\n","\n","class VQADataset(Dataset):\n","    def __init__(self, data_x, data_y, image_dict, augment=False, augment_prob=0.3, is_training=True):\n","        self.data_x = data_x\n","        self.data_y = data_y\n","        self.image_dict = image_dict\n","        self.augment = augment\n","        self.augment_prob = augment_prob  # Probability of applying augmentation\n","        self.is_training = is_training\n","\n","        # Data augmentation for training\n","        self.train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(5),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","            transforms.RandomResizedCrop(224, scale=(0.95, 1.0))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","\n","    def __getitem__(self, idx):\n","        # Tokenized text (input_ids and attention_mask)\n","        input_ids_list = torch.tensor(self.data_x['input_ids'].iloc[idx], dtype=torch.long)\n","        attention_mask_list = torch.tensor(self.data_x['attention_mask'].iloc[idx], dtype=torch.long)\n","\n","        # Image data\n","        image_id = self.data_x['image_id'].iloc[idx]\n","        image_tensor = torch.tensor(self.image_dict[image_id], dtype=torch.float32).permute(2, 0, 1)\n","\n","        # Apply augmentation\n","        if self.augment and torch.rand(1).item() < self.augment_prob:\n","            image_tensor = self.train_transform(image_tensor)\n","\n","        # Get label (correct answer)\n","        label = torch.tensor(self.data_y['label'].iloc[idx], dtype=torch.long)\n","\n","        # Only return confidence scores if in training mode\n","        if self.is_training:\n","            confidence_scores = torch.tensor(self.data_y['confidence_scores'].iloc[idx], dtype=torch.float32)\n","            return input_ids_list, attention_mask_list, image_tensor, label, confidence_scores\n","        else:\n","            return input_ids_list, attention_mask_list, image_tensor, label\n","\n","###########################################\n","# BERT-based Text Model with Dimensionality Reduction\n","###########################################\n","\n","class TextModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune=False, reduced_dim=512):\n","        super(TextModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # Optionally freeze BERT weights\n","        if not fine_tune:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","        # Dimensionality reduction after BERT\n","        self.fc_reduce = nn.Linear(768, reduced_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        batch_size, num_choices, seq_len = input_ids.size()\n","        input_ids = input_ids.view(-1, seq_len)\n","        attention_mask = attention_mask.view(-1, seq_len)\n","\n","        # Extract [CLS] token output from BERT\n","        with torch.no_grad():\n","            text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","            text_features = text_output.last_hidden_state[:, 0, :]\n","\n","        reduced_text_features = self.fc_reduce(text_features)\n","        reduced_text_features = reduced_text_features.view(batch_size, num_choices, -1)\n","        return reduced_text_features\n","\n","###########################################\n","# Image Model using VGG-16 with Customization\n","###########################################\n","\n","class ImageModel(nn.Module):\n","    def __init__(self, fine_tune_layers=True, reduced_dim=512):\n","        super(ImageModel, self).__init__()\n","        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","        # Freeze VGG layers by default\n","        for param in self.vgg16.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreeze the last layers for fine-tuning\n","        if fine_tune_layers:\n","            for param in list(self.vgg16.features.parameters())[-9:]:\n","                param.requires_grad = True\n","\n","        self.fc_reduce = nn.Linear(25088, reduced_dim)\n","\n","    def forward(self, image_tensor):\n","        image_features = self.vgg16.features(image_tensor)\n","        image_features = torch.flatten(image_features, start_dim=1)\n","        image_features = self.fc_reduce(image_features)\n","        return image_features\n","\n","###########################################\n","# Fusion Model combining Text and Image Features\n","###########################################\n","\n","class FusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(FusionModel, self).__init__()\n","\n","        self.text_model = TextModel(bert_model_name=bert_model_name, fine_tune=fine_tune_bert)\n","        self.image_model = ImageModel(fine_tune_layers=fine_tune_layers, reduced_dim=reduced_dim)\n","\n","        # Fusion layer\n","        self.fusion_layer = nn.Linear(reduced_dim + reduced_dim, 64)\n","        self.batch_norm = nn.BatchNorm1d(64)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n","        image_features = self.image_model(image_tensor=image_tensor)\n","\n","        batch_size, num_choices, _ = text_features.size()\n","        image_features = image_features.unsqueeze(1).expand(batch_size, num_choices, -1)\n","\n","        combined_features = torch.cat((text_features, image_features), dim=2)\n","        combined_features = combined_features.view(-1, combined_features.size(2))\n","        fused_features = self.fusion_layer(combined_features)\n","        fused_features = self.batch_norm(fused_features)\n","        fused_features = self.dropout(fused_features)\n","        fused_features = fused_features.view(batch_size, num_choices, -1)\n","\n","        return fused_features\n","\n","###########################################\n","# VQA Model for Final Prediction\n","###########################################\n","\n","class VQA_Model(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(VQA_Model, self).__init__()\n","        self.fusion_model = FusionModel(bert_model_name=bert_model_name, fine_tune_bert=fine_tune_bert, fine_tune_layers=fine_tune_layers, dropout_prob=dropout_prob, reduced_dim=reduced_dim)\n","        self.classifier = nn.Linear(64, 1)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor, confidence_scores=None):\n","        fused_features = self.fusion_model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_tensor)\n","        logits = self.classifier(fused_features).squeeze(-1)\n","\n","        # Adjust logits with confidence scores during training (if provided)\n","        if confidence_scores is not None:\n","            logits = logits * confidence_scores\n","\n","        batch_size, num_choices = input_ids.size(0), fused_features.size(1)\n","        logits = logits.view(batch_size, num_choices)\n","\n","        return logits\n","\n","###########################################\n","# Early Stopping Class\n","###########################################\n","\n","class EarlyStopping:\n","    def __init__(self, patience=5, delta=0, path='checkpoint.pt'):\n","        self.patience = patience\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","        self.delta = delta\n","        self.path = path\n","\n","    def __call__(self, val_loss, model):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(val_loss, model)\n","        elif val_loss > self.best_loss + self.delta:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decreases.'''\n","        torch.save(model.state_dict(), self.path)\n","\n","###########################################\n","# Training Function with Accuracy and Mixed Precision\n","###########################################\n","\n","def train_model(model, train_dataloader, val_dataloader, num_epochs=10, lr=1e-5, checkpoint_path='model_checkpoint.pt', early_stopping_patience=3):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Mixed precision scaler\n","    scaler = GradScaler()\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2)\n","\n","    early_stopping = EarlyStopping(patience=early_stopping_patience, path=checkpoint_path)\n","\n","    torch.backends.cudnn.benchmark = True  # Enable cuDNN autotuning for optimization\n","\n","    with profiler.profile(use_cuda=True) as prof:  # Enable profiling\n","        for epoch in range(num_epochs):\n","            model.train()\n","            total_loss = 0\n","            correct = 0\n","            total = 0\n","\n","            for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","                input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","                input_ids, attention_mask, image_tensor, labels, confidence_scores = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                # Mixed precision forward pass\n","                with autocast():\n","                    logits = model(input_ids, attention_mask, image_tensor, confidence_scores=confidence_scores)\n","                    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n","                    loss = loss_fn(logits, labels)\n","\n","                # Backward pass with gradient scaling\n","                scaler.scale(loss).backward()\n","\n","                # Gradient clipping\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","                # Optimizer step with mixed precision\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","                total_loss += loss.item()\n","\n","                _, predicted = torch.max(logits, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","            avg_loss = total_loss / len(train_dataloader)\n","            accuracy = 100 * correct / total\n","            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","            # Validation Loop\n","            model.eval()\n","            val_loss = 0\n","            val_correct = 0\n","            val_total = 0\n","            with torch.no_grad():\n","                for batch in val_dataloader:\n","                    input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","                    input_ids, attention_mask, image_tensor, labels, confidence_scores = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","                    logits = model(input_ids, attention_mask, image_tensor)\n","                    val_loss += loss_fn(logits, labels).item()\n","\n","                    _, predicted = torch.max(logits, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","            avg_val_loss = val_loss / len(val_dataloader)\n","            val_accuracy = 100 * val_correct / val_total\n","            print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","            scheduler.step(avg_val_loss)\n","\n","            early_stopping(avg_val_loss, model)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping triggered\")\n","                break\n","\n","    print(\"Training complete.\")\n","    print(prof.key_averages().table(sort_by=\"cuda_time_total\"))  # Print profiling results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hK4U8SNfTi_X"},"outputs":[],"source":["\n","###########################################\n","# Example Usage\n","###########################################\n","\n","# Initialize the dataset for training\n","train_dataset = VQADataset(data_x=train_x, data_y=train_y, image_dict=image_dict)\n","\n","# Initialize the dataset for validation (optional)\n","val_dataset = VQADataset(data_x=test_x, data_y=test_y, image_dict=image_dict)\n","\n","# Define Dataset and DataLoader with optimizations\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n","\n","# Define the model\n","model = VQA_Model(\n","    bert_model_name='bert-base-uncased',\n","    fine_tune_bert=True,\n","    fine_tune_layers=True,\n","    dropout_prob=0.5,\n","    reduced_dim=512\n",")\n","\n","# Train the model for 10 epochs and save checkpoints\n","train_model(\n","    model=model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    num_epochs=1000,\n","    lr=2e-5,\n","    checkpoint_path='vqa_model_checkpoint_89.pt',\n","    early_stopping_patience=10\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6uWtlO7pSUn"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertModel\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torchvision import transforms, models\n","from torchvision.models import VGG16_Weights\n","\n","###########################################\n","# Custom Dataset Class for Loading VQA Data\n","###########################################\n","\n","class VQADataset(Dataset):\n","    def __init__(self, data_x, data_y, image_dict, augment=False, augment_prob=0.3, is_training=True):\n","        self.data_x = data_x\n","        self.data_y = data_y\n","        self.image_dict = image_dict\n","        self.augment = augment\n","        self.augment_prob = augment_prob  # Probability of applying augmentation\n","        self.is_training = is_training\n","\n","        # Data augmentation for training\n","        self.train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(5),  # Reduced rotation\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Less jitter\n","            transforms.RandomResizedCrop(224, scale=(0.95, 1.0))  # Slightly less aggressive cropping\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","\n","    def __getitem__(self, idx):\n","        # Tokenized text (input_ids and attention_mask)\n","        input_ids_list = torch.tensor(self.data_x['input_ids'].iloc[idx], dtype=torch.long)\n","        attention_mask_list = torch.tensor(self.data_x['attention_mask'].iloc[idx], dtype=torch.long)\n","\n","        # Image data\n","        image_id = self.data_x['image_id'].iloc[idx]\n","        image_tensor = torch.tensor(self.image_dict[image_id], dtype=torch.float32).permute(2, 0, 1)\n","\n","        # Apply augmentation\n","        if self.augment and torch.rand(1).item() < self.augment_prob:\n","            image_tensor = self.train_transform(image_tensor)\n","\n","        # Get label (correct answer)\n","        label = torch.tensor(self.data_y['label'].iloc[idx], dtype=torch.long)\n","\n","        # Only return confidence scores if in training mode\n","        if self.is_training:\n","            confidence_scores = torch.tensor(self.data_y['confidence_scores'].iloc[idx], dtype=torch.float32)\n","            return input_ids_list, attention_mask_list, image_tensor, label, confidence_scores\n","        else:\n","            return input_ids_list, attention_mask_list, image_tensor, label\n","\n","###########################################\n","# BERT-based Text Model with Dimensionality Reduction\n","###########################################\n","\n","class TextModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune=False, reduced_dim=512):\n","        super(TextModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # Optionally freeze BERT weights\n","        if not fine_tune:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","        # Dimensionality reduction after BERT\n","        self.fc_reduce = nn.Linear(768, reduced_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        batch_size, num_choices, seq_len = input_ids.size()\n","        input_ids = input_ids.view(-1, seq_len)\n","        attention_mask = attention_mask.view(-1, seq_len)\n","\n","        # Extract [CLS] token output from BERT\n","        with torch.no_grad():\n","            text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","            text_features = text_output.last_hidden_state[:, 0, :]\n","\n","        reduced_text_features = self.fc_reduce(text_features)\n","        reduced_text_features = reduced_text_features.view(batch_size, num_choices, -1)\n","        return reduced_text_features\n","\n","###########################################\n","# Image Model using VGG-16 with Customization\n","###########################################\n","\n","class ImageModel(nn.Module):\n","    def __init__(self, fine_tune_layers=True, reduced_dim=512):\n","        super(ImageModel, self).__init__()\n","        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","        # Freeze VGG layers by default\n","        for param in self.vgg16.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreeze the last layers for fine-tuning\n","        if fine_tune_layers:\n","            for param in list(self.vgg16.features.parameters())[-9:]:\n","                param.requires_grad = True\n","\n","        self.fc_reduce = nn.Linear(25088, reduced_dim)\n","\n","    def forward(self, image_tensor):\n","        image_features = self.vgg16.features(image_tensor)\n","        image_features = torch.flatten(image_features, start_dim=1)\n","        image_features = self.fc_reduce(image_features)\n","        return image_features\n","\n","###########################################\n","# Fusion Model combining Text and Image Features\n","###########################################\n","\n","class FusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(FusionModel, self).__init__()\n","\n","        self.text_model = TextModel(bert_model_name=bert_model_name, fine_tune=fine_tune_bert)\n","        self.image_model = ImageModel(fine_tune_layers=fine_tune_layers, reduced_dim=reduced_dim)\n","\n","        # Fusion layer\n","        self.fusion_layer = nn.Linear(reduced_dim + reduced_dim, 64)\n","        self.batch_norm = nn.BatchNorm1d(64)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n","        image_features = self.image_model(image_tensor=image_tensor)\n","\n","        batch_size, num_choices, _ = text_features.size()\n","        image_features = image_features.unsqueeze(1).expand(batch_size, num_choices, -1)\n","\n","        combined_features = torch.cat((text_features, image_features), dim=2)\n","        combined_features = combined_features.view(-1, combined_features.size(2))\n","        fused_features = self.fusion_layer(combined_features)\n","        fused_features = self.batch_norm(fused_features)\n","        fused_features = self.dropout(fused_features)\n","        fused_features = fused_features.view(batch_size, num_choices, -1)\n","\n","        return fused_features\n","\n","###########################################\n","# VQA Model for Final Prediction\n","###########################################\n","\n","class VQA_Model(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(VQA_Model, self).__init__()\n","        self.fusion_model = FusionModel(bert_model_name=bert_model_name, fine_tune_bert=fine_tune_bert, fine_tune_layers=fine_tune_layers, dropout_prob=dropout_prob, reduced_dim=reduced_dim)\n","        self.classifier = nn.Linear(64, 1)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor, confidence_scores=None):\n","        fused_features = self.fusion_model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_tensor)\n","        logits = self.classifier(fused_features).squeeze(-1)\n","\n","        # Adjust logits with confidence scores during training (if provided)\n","        if confidence_scores is not None:\n","            logits = logits * confidence_scores\n","\n","        batch_size, num_choices = input_ids.size(0), fused_features.size(1)\n","        logits = logits.view(batch_size, num_choices)\n","\n","        return logits\n","\n","###########################################\n","# Training Function with Accuracy\n","###########################################\n","\n","def train_model(model, train_dataloader, val_dataloader, num_epochs=10, lr=1e-5, checkpoint_path='model_checkpoint.pt', early_stopping_patience=3):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n","\n","    early_stopping = EarlyStopping(patience=early_stopping_patience, path=checkpoint_path)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","            input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","            input_ids, attention_mask, image_tensor, labels, confidence_scores = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","            optimizer.zero_grad()\n","\n","            logits = model(input_ids, attention_mask, image_tensor, confidence_scores=confidence_scores)\n","            loss_fn = nn.CrossEntropyLoss()\n","            loss = loss_fn(logits, labels)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(logits, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        avg_loss = total_loss / len(train_dataloader)\n","        accuracy = 100 * correct / total\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","        # Validation Loop\n","        model.eval()\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids, attention_mask, image_tensor, labels, confidence_scores = batch\n","                input_ids, attention_mask, image_tensor, labels, confidence_scores = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device), confidence_scores.to(device)\n","                logits = model(input_ids, attention_mask, image_tensor)\n","                val_loss += loss_fn(logits, labels).item()\n","\n","                _, predicted = torch.max(logits, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        avg_val_loss = val_loss / len(val_dataloader)\n","        val_accuracy = 100 * val_correct / val_total\n","        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","        scheduler.step(avg_val_loss)\n","\n","        early_stopping(avg_val_loss, model)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","    print(\"Training complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o5RRVXW_l_-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7yuZvd6rLnY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yshZuj62VXS8"},"source":["### new"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRbUrUy-qrqC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertModel\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision import transforms, models\n","from torchvision.models import VGG16_Weights\n","\n","###########################################\n","# Custom Dataset Class for Loading VQA Data\n","###########################################\n","\n","class VQADataset(Dataset):\n","    def __init__(self, data_x, data_y, image_dict, augment=False, augment_prob=0.3, is_training=True):\n","        self.data_x = data_x\n","        self.data_y = data_y\n","        self.image_dict = image_dict\n","        self.augment = augment\n","        self.augment_prob = augment_prob\n","        self.is_training = is_training\n","\n","        self.train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(5),\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","            transforms.RandomResizedCrop(224, scale=(0.95, 1.0))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","\n","    def __getitem__(self, idx):\n","        input_ids_list = torch.tensor(self.data_x['input_ids'].iloc[idx], dtype=torch.long)\n","        attention_mask_list = torch.tensor(self.data_x['attention_mask'].iloc[idx], dtype=torch.long)\n","        image_id = self.data_x['image_id'].iloc[idx]\n","        image_tensor = torch.tensor(self.image_dict[image_id], dtype=torch.float32).permute(2, 0, 1)\n","\n","        if self.augment and torch.rand(1).item() < self.augment_prob:\n","            image_tensor = self.train_transform(image_tensor)\n","\n","        label = torch.tensor(self.data_y['label'].iloc[idx], dtype=torch.long)\n","\n","        return input_ids_list, attention_mask_list, image_tensor, label\n","\n","###########################################\n","# BERT-based Text Model with Freezing up to Layer 8\n","###########################################\n","\n","class TextModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune=False, reduced_dim=512):\n","        super(TextModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # Freeze the first 8 layers of BERT\n","        for param in self.bert.encoder.layer[:8].parameters():\n","            param.requires_grad = False\n","\n","        # Dimensionality reduction after BERT\n","        self.fc_reduce = nn.Linear(768, reduced_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        batch_size, num_choices, seq_len = input_ids.size()\n","        input_ids = input_ids.view(-1, seq_len)\n","        attention_mask = attention_mask.view(-1, seq_len)\n","\n","        text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_features = text_output.last_hidden_state[:, 0, :]\n","\n","        reduced_text_features = self.fc_reduce(text_features)\n","        reduced_text_features = reduced_text_features.view(batch_size, num_choices, -1)\n","        return reduced_text_features\n","\n","###########################################\n","# Image Model using VGG-16 with Customization\n","###########################################\n","\n","class ImageModel(nn.Module):\n","    def __init__(self, fine_tune_layers=True, reduced_dim=512):\n","        super(ImageModel, self).__init__()\n","        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","        # Freeze VGG layers by default\n","        for param in self.vgg16.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreeze the last layers for fine-tuning\n","        if fine_tune_layers:\n","            for param in list(self.vgg16.features.parameters())[-5:]:\n","                param.requires_grad = True\n","\n","        self.fc_reduce = nn.Linear(25088, reduced_dim)\n","\n","    def forward(self, image_tensor):\n","        image_features = self.vgg16.features(image_tensor)\n","        image_features = torch.flatten(image_features, start_dim=1)\n","        image_features = self.fc_reduce(image_features)\n","        return image_features\n","\n","###########################################\n","# Fusion Model combining Text and Image Features\n","###########################################\n","\n","class FusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(FusionModel, self).__init__()\n","\n","        self.text_model = TextModel(bert_model_name=bert_model_name, fine_tune=fine_tune_bert)\n","        self.image_model = ImageModel(fine_tune_layers=fine_tune_layers, reduced_dim=reduced_dim)\n","\n","        # Fusion layer\n","        self.fusion_layer = nn.Linear(reduced_dim + reduced_dim, 64)\n","        self.batch_norm = nn.BatchNorm1d(64)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n","        image_features = self.image_model(image_tensor=image_tensor)\n","\n","        batch_size, num_choices, _ = text_features.size()\n","        image_features = image_features.unsqueeze(1).expand(batch_size, num_choices, -1)\n","\n","        combined_features = torch.cat((text_features, image_features), dim=2)\n","        combined_features = combined_features.view(-1, combined_features.size(2))\n","        fused_features = self.fusion_layer(combined_features)\n","        fused_features = self.batch_norm(fused_features)\n","        fused_features = self.dropout(fused_features)\n","        fused_features = fused_features.view(batch_size, num_choices, -1)\n","\n","        return fused_features\n","\n","###########################################\n","# VQA Model for Final Prediction\n","###########################################\n","\n","class VQA_Model(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(VQA_Model, self).__init__()\n","        self.fusion_model = FusionModel(bert_model_name=bert_model_name, fine_tune_bert=fine_tune_bert, fine_tune_layers=fine_tune_layers, dropout_prob=dropout_prob, reduced_dim=reduced_dim)\n","        self.classifier = nn.Linear(64, 1)\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        fused_features = self.fusion_model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_tensor)\n","        logits = self.classifier(fused_features).squeeze(-1)\n","\n","        batch_size, num_choices = input_ids.size(0), fused_features.size(1)\n","        logits = logits.view(batch_size, num_choices)\n","\n","        return logits\n","\n","###########################################\n","# Training Function\n","###########################################\n","\n","\n","def train_model(model, train_dataloader, val_dataloader, num_epochs=10, optimizer=None, checkpoint_path='model_checkpoint.pt', summary_path='training_summary.txt'):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    if optimizer is None:\n","        raise ValueError(\"Optimizer must be provided\")\n","\n","    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    # Create a folder for checkpoints if it doesn't exist\n","    if not os.path.exists(os.path.dirname(checkpoint_path)):\n","        os.makedirs(os.path.dirname(checkpoint_path))\n","\n","    torch.backends.cudnn.benchmark = True  # Enable cuDNN autotuning\n","\n","    # Open summary file\n","    with open(summary_path, 'w') as summary_file:\n","        for epoch in range(num_epochs):\n","            model.train()\n","            total_loss = 0\n","            correct = 0\n","            total = 0\n","\n","            for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","                input_ids, attention_mask, image_tensor, labels = batch\n","                input_ids, attention_mask, image_tensor, labels = (\n","                    input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device)\n","                )\n","\n","                optimizer.zero_grad()\n","\n","                logits = model(input_ids, attention_mask, image_tensor)\n","                loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n","                loss = loss_fn(logits, labels)\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                _, predicted = torch.max(logits, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","            avg_loss = total_loss / len(train_dataloader)\n","            accuracy = 100 * correct / total\n","            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","            # Save model with training loss & accuracy\n","            checkpoint_file = f\"{checkpoint_path}_epoch_{epoch + 1}_trainloss_{avg_loss:.4f}_trainacc_{accuracy:.2f}.pt\"\n","            torch.save(model.state_dict(), checkpoint_file)\n","            print(f\"Model saved to {checkpoint_file}\")\n","\n","            # Write to summary file\n","            summary_file.write(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\\n\")\n","\n","            # Only run validation every 3 epochs\n","            if (epoch + 1) % 3 == 0:\n","                model.eval()\n","                val_loss = 0\n","                val_correct = 0\n","                val_total = 0\n","                with torch.no_grad():\n","                    for batch in val_dataloader:\n","                        input_ids, attention_mask, image_tensor, labels = batch\n","                        input_ids, attention_mask, image_tensor, labels = (\n","                            input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device)\n","                        )\n","                        logits = model(input_ids, attention_mask, image_tensor)\n","                        val_loss += loss_fn(logits, labels).item()\n","\n","                        _, predicted = torch.max(logits, 1)\n","                        val_total += labels.size(0)\n","                        val_correct += (predicted == labels).sum().item()\n","\n","                avg_val_loss = val_loss / len(val_dataloader)\n","                val_accuracy = 100 * val_correct / val_total\n","                print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","                # Write validation scores to summary file\n","                summary_file.write(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n","\n","                # Save model with validation score\n","                checkpoint_file = f\"{checkpoint_path}_epoch_{epoch + 1}_trainloss_{avg_loss:.4f}_trainacc_{accuracy:.2f}_valloss_{avg_val_loss:.4f}_valacc_{val_accuracy:.2f}.pt\"\n","                torch.save(model.state_dict(), checkpoint_file)\n","                print(f\"Model saved to {checkpoint_file} with validation scores\")\n","\n","                scheduler.step()\n","\n","    print(\"Training complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7bab4cdbffa1429fa7b9002a47eba043","ab659a5bff2e49a19fd4dfd1fbcd90a5","eff66340078040b5a18ce3379d0b2c52","44cc3eed0bbb45528f431d1818761823","a855b828e39341409f3188c4db79c803","eeb7be4af7164de69d4d943085366b31","fb4373f5674e4e9aa86cde0b9e5fe4f5","f5c054fdedae47268889d2bec9b59480","748173046af84051a4e3ee9f71deebf5","4dd3cee6da6c4d728aba2a8b497b5b01","5231c405fad04ce4a20efdf4cae49169","9adc7f761b6b4c71bda41d078ccde131","74c6445854df4242bda640b40ee05aef","025eefe9b43e41628d0a2563ad1a5298","05b85f152cd7455aab9b34448b3591ba","fe1dbd42c70b4a54bbe7652988dd08e0","92d18f6d719e4700ba161d585c0d5531","a2d28ef0ce2c45b1b2cb3a0d3c7f769f","707ec154c0894dbcac4df4bd70e84a03","24b5369eb5c845c9a44522050aad3b43","6e8fa9a7d7be418784c96f12ffb20659","e6994f3033d945a68d672b5450504643"]},"id":"8AQ5agraTc8p","outputId":"eba78cc4-fbf9-4168-ace7-ca51fe46f8c0"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bab4cdbffa1429fa7b9002a47eba043","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9adc7f761b6b4c71bda41d078ccde131","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|| 528M/528M [00:02<00:00, 217MB/s]\n","Epoch 1/1000: 100%|| 3000/3000 [45:36<00:00,  1.10it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/1000, Loss: 2.0465, Accuracy: 33.18%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_1_trainloss_2.0465_trainacc_33.18.pt\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2/1000, Loss: 1.7838, Accuracy: 39.74%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_2_trainloss_1.7838_trainacc_39.74.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/1000, Loss: 1.7205, Accuracy: 42.06%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_3_trainloss_1.7205_trainacc_42.06.pt\n","Validation Loss: 1.6016, Validation Accuracy: 48.41%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_3_trainloss_1.7205_trainacc_42.06_valloss_1.6016_valacc_48.41.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/1000, Loss: 1.6795, Accuracy: 43.92%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_4_trainloss_1.6795_trainacc_43.92.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/1000, Loss: 1.6498, Accuracy: 45.56%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_5_trainloss_1.6498_trainacc_45.56.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/1000, Loss: 1.6248, Accuracy: 46.77%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_6_trainloss_1.6248_trainacc_46.77.pt\n","Validation Loss: 1.5240, Validation Accuracy: 52.78%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_6_trainloss_1.6248_trainacc_46.77_valloss_1.5240_valacc_52.78.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/1000, Loss: 1.5969, Accuracy: 48.45%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_7_trainloss_1.5969_trainacc_48.45.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/1000, Loss: 1.5802, Accuracy: 49.25%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_8_trainloss_1.5802_trainacc_49.25.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/1000, Loss: 1.5592, Accuracy: 50.34%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_9_trainloss_1.5592_trainacc_50.34.pt\n","Validation Loss: 1.4765, Validation Accuracy: 54.86%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_9_trainloss_1.5592_trainacc_50.34_valloss_1.4765_valacc_54.86.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/1000, Loss: 1.5420, Accuracy: 51.21%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_10_trainloss_1.5420_trainacc_51.21.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/1000, Loss: 1.5296, Accuracy: 51.81%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_11_trainloss_1.5296_trainacc_51.81.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/1000, Loss: 1.5185, Accuracy: 52.62%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_12_trainloss_1.5185_trainacc_52.62.pt\n","Validation Loss: 1.4418, Validation Accuracy: 56.48%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_12_trainloss_1.5185_trainacc_52.62_valloss_1.4418_valacc_56.48.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/1000, Loss: 1.5054, Accuracy: 52.89%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_13_trainloss_1.5054_trainacc_52.89.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/1000, Loss: 1.4962, Accuracy: 53.37%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_14_trainloss_1.4962_trainacc_53.37.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/1000, Loss: 1.4834, Accuracy: 53.94%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_15_trainloss_1.4834_trainacc_53.94.pt\n","Validation Loss: 1.4282, Validation Accuracy: 57.52%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_15_trainloss_1.4834_trainacc_53.94_valloss_1.4282_valacc_57.52.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/1000, Loss: 1.4702, Accuracy: 55.09%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_16_trainloss_1.4702_trainacc_55.09.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/1000: 100%|| 3000/3000 [45:34<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/1000, Loss: 1.4714, Accuracy: 54.52%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_17_trainloss_1.4714_trainacc_54.52.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/1000, Loss: 1.4706, Accuracy: 54.73%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_18_trainloss_1.4706_trainacc_54.73.pt\n","Validation Loss: 1.4160, Validation Accuracy: 58.58%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_18_trainloss_1.4706_trainacc_54.73_valloss_1.4160_valacc_58.58.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/1000, Loss: 1.4694, Accuracy: 54.88%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_19_trainloss_1.4694_trainacc_54.88.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/1000, Loss: 1.4655, Accuracy: 55.05%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_20_trainloss_1.4655_trainacc_55.05.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/1000, Loss: 1.4656, Accuracy: 55.19%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_21_trainloss_1.4656_trainacc_55.19.pt\n","Validation Loss: 1.4171, Validation Accuracy: 58.64%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_21_trainloss_1.4656_trainacc_55.19_valloss_1.4171_valacc_58.64.pt with validation scores\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/1000: 100%|| 3000/3000 [45:33<00:00,  1.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/1000, Loss: 1.4679, Accuracy: 54.92%\n","Model saved to /content/drive/MyDrive/Colab Notebooks/Deep Learning Project/vqa_model_checkpoint_with_confidence.pt_epoch_22_trainloss_1.4679_trainacc_54.92.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/1000:  66%|   | 1993/3000 [30:16<15:17,  1.10it/s]"]}],"source":["###########################################\n","# Example Usage\n","###########################################\n","\n","import os\n","\n","# Initialize the dataset for training\n","train_dataset = VQADataset(data_x=train_x, data_y=train_y, image_dict=image_dict)\n","\n","# Initialize the dataset for validation (optional)\n","val_dataset = VQADataset(data_x=test_x, data_y=test_y, image_dict=image_dict)\n","\n","# Define Dataset and DataLoader with optimizations\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n","\n","# Define the model\n","model = VQA_Model(\n","    bert_model_name='bert-base-uncased',\n","    fine_tune_bert=True,\n","    fine_tune_layers=True,\n","    dropout_prob=0.5,\n","    reduced_dim=512\n",")\n","\n","# Optimizer setup with adjusted learning rates for BERT and VGG layers\n","optimizer = optim.AdamW([\n","    {'params': model.fusion_model.text_model.bert.parameters(), 'lr': 1e-6},  # Lower learning rate for BERT\n","    {'params': model.fusion_model.image_model.parameters(), 'lr': 5e-5},     # Lower learning rate for VGG layers\n","    {'params': model.fusion_model.fusion_layer.parameters(), 'lr': 1e-4},    # Learning rate for the fusion layer\n","    {'params': model.classifier.parameters(), 'lr': 1e-4}                    # Learning rate for classifier\n","], weight_decay=1e-4)\n","\n","# Path to save checkpoints and training summaries on Google Drive\n","save_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Project'\n","checkpoint_path = f'{save_dir}/vqa_model_checkpoint_with_confidence.pt'\n","summary_path = f'{save_dir}/training_summary.txt'\n","\n","# Train the model with the new optimizer and checkpoint saving configuration\n","train_model(\n","    model=model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    num_epochs=1000,\n","    optimizer=optimizer,\n","    checkpoint_path=checkpoint_path,  # Save checkpoints to Google Drive\n","    summary_path=summary_path  # Save the training and validation summary to Google Drive\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"XEfufcMvXrDh"},"source":["### Older"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNPLA7dXIw4L"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertModel\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torchvision import transforms, models\n","from torchvision.models import VGG16_Weights\n","\n","###########################################\n","# Custom Dataset Class for Loading VQA Data\n","###########################################\n","\n","class VQADataset(Dataset):\n","    def __init__(self, data_x, data_y, image_dict, augment=False, augment_prob=0.3):\n","        self.data_x = data_x\n","        self.data_y = data_y\n","        self.image_dict = image_dict\n","        self.augment = augment\n","        self.augment_prob = augment_prob  # Probability of applying augmentation\n","\n","        # Less aggressive data augmentation\n","        self.train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(5),  # Reduced rotation\n","            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Less intense jitter\n","            transforms.RandomResizedCrop(224, scale=(0.95, 1.0))  # Less cropping\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","\n","    def __getitem__(self, idx):\n","        # Get all input_ids and attention masks for the 18 options\n","        input_ids_list = torch.tensor(self.data_x['input_ids'].iloc[idx], dtype=torch.long)\n","        attention_mask_list = torch.tensor(self.data_x['attention_mask'].iloc[idx], dtype=torch.long)\n","\n","        # Get image tensor and permute for correct dimensions (channels first)\n","        image_id = self.data_x['image_id'].iloc[idx]\n","        image_tensor = torch.tensor(self.image_dict[image_id], dtype=torch.float32).permute(2, 0, 1)\n","\n","        # Apply data augmentation with a certain probability\n","        if self.augment and torch.rand(1).item() < self.augment_prob:\n","            image_tensor = self.train_transform(image_tensor)\n","\n","        # Get label (index of the correct answer)\n","        label = torch.tensor(self.data_y['label'].iloc[idx], dtype=torch.long)\n","\n","        return input_ids_list, attention_mask_list, image_tensor, label\n","\n","###########################################\n","# BERT Model for Text Processing (with Fine-Tuning)\n","###########################################\n","\n","class TextModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune=True, reduced_dim=512):\n","        \"\"\"\n","        TextModel using BERT with dimensionality reduction.\n","\n","        Args:\n","        - bert_model_name (str): Pretrained BERT model name.\n","        - fine_tune (bool): If True, all BERT layers are trainable. If False, they are frozen.\n","        - reduced_dim (int): Dimension to reduce BERT output to (default is 512).\n","        \"\"\"\n","        super(TextModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        # If fine-tune is set to False, freeze all BERT layers\n","        if not fine_tune:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        else:\n","            # Unfreeze all layers by ensuring requires_grad=True\n","            for param in self.bert.parameters():\n","                param.requires_grad = True\n","\n","        # Add a fully connected layer to reduce dimensionality of BERT's output\n","        self.fc_reduce = nn.Linear(768, reduced_dim)  # 768 is the BERT hidden size\n","\n","    def forward(self, input_ids, attention_mask):\n","        batch_size, num_choices, seq_len = input_ids.size()\n","        input_ids = input_ids.view(-1, seq_len)  # Flatten batch and choices for BERT\n","        attention_mask = attention_mask.view(-1, seq_len)\n","\n","        # Get BERT outputs (allow gradients to be computed if fine-tune=True)\n","        text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_features = text_output.last_hidden_state[:, 0, :]  # Extract [CLS] token output (batch_size * num_choices, 768)\n","\n","        # Reduce dimensions from 768 to reduced_dim\n","        reduced_text_features = self.fc_reduce(text_features)  # (batch_size * num_choices, reduced_dim)\n","\n","        # Reshape back to original shape (batch_size, num_choices, reduced_dim)\n","        reduced_text_features = reduced_text_features.view(batch_size, num_choices, -1)\n","        return reduced_text_features\n","\n","\n","\n","\n","###########################################\n","# Image Model (VGG-16) with Freezing and Unfreezing Layers\n","###########################################\n","\n","class ImageModel(nn.Module):\n","    def __init__(self, fine_tune_layers=True, reduced_dim=512):\n","        super(ImageModel, self).__init__()\n","\n","        # Load pre-trained VGG-16 with weights\n","        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","        # Freeze all layers by default\n","        for param in self.vgg16.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreeze the last few layers for fine-tuning\n","        if fine_tune_layers:\n","            for param in list(self.vgg16.features.parameters())[-9:]:\n","                param.requires_grad = True\n","\n","        # Use a linear layer to reduce dimensionality of the image features\n","        self.reduced_dim = reduced_dim\n","        self.fc_reduce = nn.Linear(25088, reduced_dim)\n","\n","    def forward(self, image_tensor):\n","        # Extract image features from VGG-16 and flatten the output\n","        image_features = self.vgg16.features(image_tensor)  # Output is 512 x 7 x 7\n","        image_features = torch.flatten(image_features, start_dim=1)  # Flatten to (batch_size, 25088)\n","\n","        # Reduce dimensionality\n","        image_features = self.fc_reduce(image_features)  # Now the output is (batch_size, reduced_dim)\n","        return image_features\n","\n","###########################################\n","# Fusion Model (Text + Image)\n","###########################################\n","\n","class FusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(FusionModel, self).__init__()\n","\n","        # Load text and image models\n","        self.text_model = TextModel(bert_model_name=bert_model_name, fine_tune=fine_tune_bert)\n","        self.image_model = ImageModel(fine_tune_layers=fine_tune_layers, reduced_dim=reduced_dim)\n","\n","        # Fusion layers for combining text and image features\n","        fusion_input_size = reduced_dim + reduced_dim  # Concatenated size of text and reduced image features\n","        self.fusion_layer = nn.Linear(fusion_input_size, 64)  # Reduce from large to smaller size\n","        self.batch_norm = nn.BatchNorm1d(64)  # Batch Normalization\n","        self.dropout = nn.Dropout(dropout_prob)  # Dropout for regularization\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        # Extract text features for all choices\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)  # (batch_size, num_choices, 768)\n","\n","        # Extract image features\n","        image_features = self.image_model(image_tensor=image_tensor)  # (batch_size, reduced_dim)\n","\n","        # Expand image features to match the number of choices\n","        batch_size, num_choices, _ = text_features.size()\n","        image_features = image_features.unsqueeze(1).expand(batch_size, num_choices, -1)  # (batch_size, num_choices, reduced_dim)\n","\n","        # Concatenate text and image features\n","        combined_features = torch.cat((text_features, image_features), dim=2)  # (batch_size, num_choices, fusion_input_size)\n","\n","        # Apply fusion layer to each choice\n","        combined_features = combined_features.view(-1, combined_features.size(2))  # Flatten for linear layer\n","        fused_features = self.fusion_layer(combined_features)\n","        fused_features = self.batch_norm(fused_features)\n","        fused_features = self.dropout(fused_features)\n","\n","        # Reshape back to (batch_size, num_choices, fusion_output_size)\n","        fused_features = fused_features.view(batch_size, num_choices, -1)\n","\n","        return fused_features\n","\n","###########################################\n","# VQA Model\n","###########################################\n","\n","class VQA_Model(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', fine_tune_bert=True, fine_tune_layers=True, dropout_prob=0.3, reduced_dim=512):\n","        super(VQA_Model, self).__init__()\n","\n","        # Combine text and image features with the FusionModel\n","        self.fusion_model = FusionModel(bert_model_name=bert_model_name, fine_tune_bert=fine_tune_bert, fine_tune_layers=fine_tune_layers, dropout_prob=dropout_prob, reduced_dim=reduced_dim)\n","\n","        # Final classification layer: A single logit per choice\n","        self.classifier = nn.Linear(64, 1)  # Single logit per choice\n","\n","    def forward(self, input_ids, attention_mask, image_tensor):\n","        # Get fused text and image features\n","        fused_features = self.fusion_model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_tensor)\n","\n","        # Apply the classifier to generate logits for each choice\n","        logits = self.classifier(fused_features).squeeze(-1)  # (batch_size * num_choices)\n","\n","        # Reshape back to (batch_size, num_choices) to compare across the options for each question\n","        batch_size, num_choices = input_ids.size(0), fused_features.size(1)\n","        logits = logits.view(batch_size, num_choices)\n","\n","        return logits\n","\n","###########################################\n","# Early Stopping Class\n","###########################################\n","\n","class EarlyStopping:\n","    def __init__(self, patience=5, delta=0, path='checkpoint.pt'):\n","        self.patience = patience\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","        self.delta = delta\n","        self.path = path\n","\n","    def __call__(self, val_loss, model):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(val_loss, model)\n","        elif val_loss > self.best_loss + self.delta:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        torch.save(model.state_dict(), self.path)\n","\n","###########################################\n","# Training Function\n","###########################################\n","\n","def train_model(model, train_dataloader, val_dataloader, num_epochs=10, lr=1e-5, checkpoint_path='model_checkpoint.pt', early_stopping_patience=3):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Optimizer and Learning Rate Scheduler\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n","\n","    # Early stopping\n","    early_stopping = EarlyStopping(patience=early_stopping_patience, path=checkpoint_path)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        # Training Loop\n","        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","            input_ids, attention_mask, image_tensor, labels = batch\n","            input_ids, attention_mask, image_tensor, labels = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            logits = model(input_ids, attention_mask, image_tensor)\n","            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n","            loss = loss_fn(logits, labels)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids, attention_mask, image_tensor, labels = batch\n","                input_ids, attention_mask, image_tensor, labels = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device)\n","                logits = model(input_ids, attention_mask, image_tensor)\n","                val_loss += loss_fn(logits, labels).item()\n","\n","        avg_val_loss = val_loss / len(val_dataloader)\n","        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n","        scheduler.step(avg_val_loss)\n","\n","        # Early stopping check\n","        early_stopping(avg_val_loss, model)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","    print(\"Training complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BSUdh-K6KmD"},"outputs":[],"source":["# Assuming train_x is your original DataFrame\n","sampled_train_x = train_x.sample(n=9600, random_state=42)  # random_state is optional for reproducibility\n","sampled_test_x = test_x.sample(n=2400, random_state=42)  # random_state is optional for reproducibility\n","sampled_train_y = train_y.sample(n=9600, random_state=42)  # random_state is optional for reproducibility\n","sampled_test_y = test_y.sample(n=2400, random_state=42)  # random_state is optional for reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"OjRFarcK6Wdj","outputId":"37fb12a8-a856-43f7-d81d-f7bd2c18c927"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/500: 100%|| 600/600 [11:42<00:00,  1.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/500, Loss: 1.5121\n","Validation Loss: 1.1904\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/500:  64%|   | 387/600 [07:34<04:09,  1.17s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-105421433cbb>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the model for 10 epochs and save checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-80-d52e6d0b87c3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr, checkpoint_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["###########################################\n","# Example Usage\n","###########################################\n","\n","# Initialize the dataset for training\n","train_dataset = VQADataset(data_x=sampled_train_x, data_y=sampled_train_y, image_dict=image_dict)\n","\n","# Initialize the dataset for validation (optional)\n","val_dataset = VQADataset(data_x=sampled_test_x, data_y=sampled_test_y, image_dict=image_dict)\n","\n","# Define Dataset and DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","# Define the model\n","model = VQA_Model(\n","    bert_model_name='bert-base-uncased',  # Pretrained BERT model\n","    fine_tune_bert=True,                 # Fine-tune BERT\n","    fine_tune_layers=True,               # Fine-tune layers of the image model\n","    dropout_prob=0.5,                    # Dropout for regularization\n","    reduced_dim=512                      # Dimension reduction in both text and image models\n",")\n","\n","# Train the model for 10 epochs and save checkpoints\n","train_model(\n","    model=model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,  # Optional: Pass validation DataLoader\n","    num_epochs=500,                  # Number of epochs\n","    lr=1e-5,                        # Learning rate (adjust as needed)\n","    checkpoint_path='vqa_model_checkpoint_v6.pt',  # Path to save the model checkpoints\n","    early_stopping_patience=10      # Early stopping patience\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Eg_9yNp5wz3W","outputId":"838aa034-5d44-4e84-db26-17b01d477da1"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-117-2d0d8eedba94>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n","Epoch 1/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5000, Loss: 0.9122\n","Validation Loss: 0.9351\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5000, Loss: 0.9046\n","Validation Loss: 0.9351\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5000, Loss: 0.9013\n","Validation Loss: 0.9291\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5000, Loss: 0.9006\n","Validation Loss: 0.9282\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5000, Loss: 0.8969\n","Validation Loss: 0.9279\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/5000, Loss: 0.8988\n","Validation Loss: 0.9241\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/5000, Loss: 0.8993\n","Validation Loss: 0.9306\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/5000, Loss: 0.8974\n","Validation Loss: 0.9228\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/5000, Loss: 0.8950\n","Validation Loss: 0.9301\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/5000, Loss: 0.8962\n","Validation Loss: 0.9241\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/5000, Loss: 0.8948\n","Validation Loss: 0.9269\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/5000, Loss: 0.8963\n","Validation Loss: 0.9262\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/5000, Loss: 0.8950\n","Validation Loss: 0.9263\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/5000, Loss: 0.8943\n","Validation Loss: 0.9261\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 15/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/5000, Loss: 0.8923\n","Validation Loss: 0.9262\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 16/5000: 100%|| 750/750 [19:17<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16/5000, Loss: 0.8915\n","Validation Loss: 0.9264\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 17/5000:   0%|          | 3/750 [00:06<25:35,  2.06s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-2d0d8eedba94>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Continue training from this state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-116-091be07960bf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr, checkpoint_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","\n","# Initialize the dataset for training\n","train_dataset = VQADataset(data_x=train_x, data_y=train_y, image_dict=image_dict)\n","\n","# Initialize the dataset for validation (optional)\n","val_dataset = VQADataset(data_x=test_x, data_y=test_y, image_dict=image_dict)\n","\n","# Define Dataset and DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","# Define the model (same architecture as the one used during training)\n","model = VQA_Model(\n","    bert_model_name='bert-base-uncased',  # Pretrained BERT model\n","    fine_tune_bert=False,                 # Freeze BERT layers\n","    fine_tune_layers=True,                # Fine-tune layers of the image model\n","    dropout_prob=0.2,                     # Dropout for regularization\n","    reduced_dim=512                       # Dimension reduction in both text and image models\n",")\n","\n","# Load the checkpoint (use the appropriate device, CPU or GPU)\n","checkpoint_path = 'vqa_model_checkpoint.pt'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the checkpoint file\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","# Load the model weights from the checkpoint\n","model.load_state_dict(checkpoint)\n","\n","# Move the model to the correct device\n","model = model.to(device)\n","\n","# Now the model is loaded with the checkpoint weights and ready for use\n","# Define the optimizer (same settings as used during training)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n","\n","# Optionally load other state information (e.g., epoch)\n","epoch = checkpoint.get('epoch', 0)\n","\n","# Continue training from this state\n","train_model(\n","    model=model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    num_epochs=5000,  # Adjust based on how much training is left\n","    lr=1e-4,\n","    checkpoint_path='vqa_model_checkpoint_v8.pt',\n","    early_stopping_patience=10\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"GZLksacY5Nmu","outputId":"2d6182f0-419b-434e-e820-022e0361e6a6"},"outputs":[{"ename":"NameError","evalue":"name 'num_epochs' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-0cb138ecdbee>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Training Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"]}],"source":["# Global variables to store training progress\n","global_accuracy = 0\n","global_num_correct = 0\n","global_total_samples = 0\n","global_batch_index = 0\n","\n","# Training Loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, image_tensor, labels = batch\n","        input_ids, attention_mask, image_tensor, labels = input_ids.to(device), attention_mask.to(device), image_tensor.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(input_ids, attention_mask, image_tensor)\n","        loss = nn.CrossEntropyLoss()(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy for this batch\n","        preds = torch.argmax(logits, dim=1)\n","        num_correct = (preds == labels).sum().item()\n","        total_samples += labels.size(0)\n","        correct_predictions += num_correct\n","\n","        # Update global accuracy\n","        global_num_correct = correct_predictions\n","        global_total_samples = total_samples\n","        global_accuracy = correct_predictions / total_samples\n","        global_batch_index += 1\n","\n","        # If you want to periodically display the accuracy every few batches, you could add a condition here.\n","import time\n","from IPython.display import clear_output\n","\n","while global_batch_index < len(train_dataloader) * num_epochs:\n","    # Clear previous output\n","    clear_output(wait=True)\n","\n","    # Print the current accuracy\n","    print(f\"Training Progress - Batch {global_batch_index}:\")\n","    print(f\"Accuracy: {global_accuracy * 100:.2f}%\")\n","\n","    # Wait for some time before printing again to avoid overloading the output\n","    time.sleep(2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uNCghpn27nOd","outputId":"62799605-907a-4a57-83fa-54a5baefdc9f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5000, Loss: 2.2676\n","Validation Loss: 1.8259\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5000, Loss: 2.2025\n","Validation Loss: 1.8054\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5000, Loss: 2.1833\n","Validation Loss: 1.7972\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5000, Loss: 2.1868\n","Validation Loss: 1.8011\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5000, Loss: 2.1771\n","Validation Loss: 1.8048\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/5000, Loss: 2.1710\n","Validation Loss: 1.8053\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/5000, Loss: 2.1740\n","Validation Loss: 1.7970\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/5000, Loss: 2.1609\n","Validation Loss: 1.7850\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/5000, Loss: 2.1641\n","Validation Loss: 1.7874\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/5000, Loss: 2.1492\n","Validation Loss: 1.7861\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/5000, Loss: 2.1664\n","Validation Loss: 1.7914\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/5000, Loss: 2.1561\n","Validation Loss: 1.7865\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/5000, Loss: 2.1591\n","Validation Loss: 1.7894\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/5000, Loss: 2.1576\n","Validation Loss: 1.7877\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 15/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/5000, Loss: 2.1644\n","Validation Loss: 1.7873\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 16/5000: 100%|| 750/750 [19:16<00:00,  1.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16/5000, Loss: 2.1608\n","Validation Loss: 1.7891\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 17/5000:  53%|    | 397/750 [10:13<09:05,  1.55s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-3004ae065ea4>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the model for 10 epochs and save checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-96-2ee49b88ab98>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr, checkpoint_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["###########################################\n","# Example Usage\n","###########################################\n","\n","# Initialize the dataset for training\n","train_dataset = VQADataset(data_x=train_x, data_y=train_y, image_dict=image_dict)\n","\n","# Initialize the dataset for validation (optional)\n","val_dataset = VQADataset(data_x=test_x, data_y=test_y, image_dict=image_dict)\n","\n","# Define Dataset and DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","# Define the model\n","model = VQA_Model(\n","    bert_model_name='bert-base-uncased',  # Pretrained BERT model\n","    fine_tune_bert=False,                 # Fine-tune BERT\n","    fine_tune_layers=True,               # Fine-tune layers of the image model\n","    dropout_prob=0.3,                    # Dropout for regularization\n","    reduced_dim=512                      # Dimension reduction in both text and image models\n",")\n","\n","# Train the model for 10 epochs and save checkpoints\n","train_model(\n","    model=model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,  # Optional: Pass validation DataLoader\n","    num_epochs=5000,                  # Number of epochs\n","    lr=1e-4,                        # Learning rate (adjust as needed)\n","    checkpoint_path='vqa_model_checkpoint_v7_fullset.pt',  # Path to save the model checkpoints\n","    early_stopping_patience=10       # Early stopping patience\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHLy9kO0Xu3S"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dSt8xh5nXvOW"},"source":["### Memoery Clean"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joQOva2YaPYJ","outputId":"c5d1b24b-22ef-4c59-984a-46907a591122"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of CPU cores: 8\n"]}],"source":["import multiprocessing\n","\n","# Get the number of CPU cores\n","num_cores = multiprocessing.cpu_count()\n","print(f\"Number of CPU cores: {num_cores}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnPULlgBYvQc","outputId":"aba6b5fb-89b1-428e-93be-777ee9371072"},"outputs":[{"name":"stdout","output_type":"stream","text":["USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n","root           1  0.0  0.0   1076     8 ?        Ss   01:27   0:00 /sbin/docker-init -- /datalab/run\n","root           7 11.6  0.0 953464 81772 ?        Sl   01:27   6:25 /tools/node/bin/node /datalab/web\n","root           9  0.0  0.0   7376  3488 ?        S    01:27   0:01 /bin/bash -e /usr/local/colab/bin\n","root          11  0.0  0.0   7376  1940 ?        S    01:27   0:00 /bin/bash -e /datalab/run.sh\n","root          12  0.0  0.0 1238116 15280 ?       Sl   01:27   0:01 /usr/colab/bin/kernel_manager_pro\n","root          31  0.0  0.0   5808  1048 ?        Ss   01:27   0:00 tail -n +0 -F /root/.config/Googl\n","root          37  0.0  0.0   5808  1012 ?        Ss   01:27   0:00 tail -n +0 -F /root/.config/Googl\n","root          89  0.2  0.0      0     0 ?        Z    01:27   0:06 [python3] <defunct>\n","root          90  0.0  0.0  67740 52384 ?        S    01:27   0:00 python3 /usr/local/bin/colab-file\n","root         139  0.2  0.2 1065900 176912 ?      Sl   01:27   0:07 /usr/bin/python3 /usr/local/bin/j\n","root         140  0.1  0.0 1231804 9988 ?        Sl   01:28   0:03 /usr/local/bin/dap_multiplexer --\n","root        2134 59.1 60.8 63000892 53300868 ?   Ssl  01:35  27:52 /usr/bin/python3 -m colab_kernel_\n","root        2175  0.2  0.0 541332 15764 ?        Sl   01:35   0:06 /usr/bin/python3 /usr/local/lib/p\n","root        2225  0.0  0.0   4364   276 ?        S    01:36   0:00 /bin/bash --noediting -i\n","root        2226  0.0  0.0 3258424 36940 ?       SLl  01:36   0:00 /opt/google/drive/drive --feature\n","root        2227  0.0  0.0   3632  1648 ?        S    01:36   0:00 grep --color=auto --line-buffered\n","root        2461  8.0  0.1 3745768 102408 ?      SLl  01:36   3:44 /opt/google/drive/drive --feature\n","root        2571  0.0  0.0   4364  3172 ?        S    01:36   0:00 bash -c tail -n +0 -F \"/root/.con\n","root        2572  0.0  0.0 1228632 3472 ?        Sl   01:36   0:00 /opt/google/drive/directoryprefet\n","root        2573  0.0  0.0   2824  1000 ?        S    01:36   0:00 tail -n +0 -F /root/.config/Googl\n","root        2574  0.0  0.0  18024  9480 ?        S    01:36   0:00 python3 /opt/google/drive/drive-f\n","root        2583  0.0  0.0 1242120 26640 ?       Sl   01:36   0:02 /usr/colab/bin/language_service -\n","root        2598  4.3  0.8 1461724 708500 ?      Sl   01:36   2:01 node /datalab/web/pyright/pyright\n","root       14672  0.0  0.0   5776  1028 ?        S    02:23   0:00 sleep 1\n","root       14673  0.0  0.0  10076  1552 ?        R    02:23   0:00 ps -aux\n"]}],"source":["# To see processes and memory usage\n","!ps -aux"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-eRkhVuh3mg"},"outputs":[],"source":["import torch\n","\n","# Delete your model, data, etc.\n","del train_model\n","del train_dataloader\n","del val_dataloader\n","del optimizer\n","\n","# Clear the cache\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBCJZuc925LP","outputId":"4110aed2-0bbf-4146-dfb1-f0dbee085a0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Oct 11 17:23:51 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   57C    P0              29W /  72W |  21945MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpfRBVt_33U5"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["vZ-G6pPX_hqU","XEfufcMvXrDh"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"025eefe9b43e41628d0a2563ad1a5298":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_707ec154c0894dbcac4df4bd70e84a03","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24b5369eb5c845c9a44522050aad3b43","value":440449768}},"05b85f152cd7455aab9b34448b3591ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e8fa9a7d7be418784c96f12ffb20659","placeholder":"","style":"IPY_MODEL_e6994f3033d945a68d672b5450504643","value":"440M/440M[00:02&lt;00:00,250MB/s]"}},"24b5369eb5c845c9a44522050aad3b43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44cc3eed0bbb45528f431d1818761823":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dd3cee6da6c4d728aba2a8b497b5b01","placeholder":"","style":"IPY_MODEL_5231c405fad04ce4a20efdf4cae49169","value":"570/570[00:00&lt;00:00,46.7kB/s]"}},"4dd3cee6da6c4d728aba2a8b497b5b01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5231c405fad04ce4a20efdf4cae49169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e8fa9a7d7be418784c96f12ffb20659":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"707ec154c0894dbcac4df4bd70e84a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748173046af84051a4e3ee9f71deebf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74c6445854df4242bda640b40ee05aef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92d18f6d719e4700ba161d585c0d5531","placeholder":"","style":"IPY_MODEL_a2d28ef0ce2c45b1b2cb3a0d3c7f769f","value":"model.safetensors:100%"}},"7bab4cdbffa1429fa7b9002a47eba043":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab659a5bff2e49a19fd4dfd1fbcd90a5","IPY_MODEL_eff66340078040b5a18ce3379d0b2c52","IPY_MODEL_44cc3eed0bbb45528f431d1818761823"],"layout":"IPY_MODEL_a855b828e39341409f3188c4db79c803"}},"92d18f6d719e4700ba161d585c0d5531":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9adc7f761b6b4c71bda41d078ccde131":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74c6445854df4242bda640b40ee05aef","IPY_MODEL_025eefe9b43e41628d0a2563ad1a5298","IPY_MODEL_05b85f152cd7455aab9b34448b3591ba"],"layout":"IPY_MODEL_fe1dbd42c70b4a54bbe7652988dd08e0"}},"a2d28ef0ce2c45b1b2cb3a0d3c7f769f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a855b828e39341409f3188c4db79c803":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab659a5bff2e49a19fd4dfd1fbcd90a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb7be4af7164de69d4d943085366b31","placeholder":"","style":"IPY_MODEL_fb4373f5674e4e9aa86cde0b9e5fe4f5","value":"config.json:100%"}},"e6994f3033d945a68d672b5450504643":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeb7be4af7164de69d4d943085366b31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff66340078040b5a18ce3379d0b2c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5c054fdedae47268889d2bec9b59480","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_748173046af84051a4e3ee9f71deebf5","value":570}},"f5c054fdedae47268889d2bec9b59480":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4373f5674e4e9aa86cde0b9e5fe4f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe1dbd42c70b4a54bbe7652988dd08e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}